# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10WYkhKn2l0GV06tJa4U2AkXBbht_OfYL
"""
#import packages
import tensorflow as tf
import random
from tensorflow.keras import regularizers
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
import pandas as pd
import numpy as np

def DTI_model(drug,protein):

  data = pd.read_csv("medi/Model/affinity.csv")

  np.random.seed(42)
  tf.random.set_seed(42)
  random.seed(42)

  smiles = list(data["smiles"])
  proteins = list(data["proteins"])

  # split

  split = int(0.9 * len(smiles))
  train_smiles = smiles[:split]
  train_proteins = proteins[:split]

  # Tokenize smiles
  #train
  # tokenizer_smiles = Tokenizer(char_level = True)
  # tokenizer_smiles.fit_on_texts(train_smiles)

  # word_index_smiles = tokenizer_smiles.word_index
  # vocab_size_smiles = len(word_index_smiles)

  # train_sequences_smiles = tokenizer_smiles.texts_to_sequences(train_smiles)
  # train_padded_smiles = pad_sequences(train_sequences_smiles, truncating = "post", padding = "post", maxlen = 85)

  # #test
  # test_sequences_smiles = tokenizer_smiles.texts_to_sequences(test_smiles)
  # test_padded_smiles = pad_sequences(test_sequences_smiles, truncating = "post", padding = "post", maxlen = 85)

  # Tokenize proteins
  #train
  # tokenizer_proteins = Tokenizer(char_level = True)
  # tokenizer_proteins.fit_on_texts(train_proteins)

  # word_index_proteins = tokenizer_proteins.word_index
  # vocab_size_proteins = len(word_index_proteins)

  # train_sequences_proteins = tokenizer_proteins.texts_to_sequences(train_proteins)
  # train_padded_proteins = pad_sequences(train_sequences_proteins, truncating = "post", padding = "post", maxlen = 1200)

  # #test
  # test_sequences_proteins = tokenizer_proteins.texts_to_sequences(test_proteins)
  # test_padded_proteins = pad_sequences(test_sequences_proteins, truncating = "post", padding = "post", maxlen = 1200)

  # print(vocab_size_smiles)
  # print(vocab_size_proteins)

  # train_smiles_array = np.array(train_padded_smiles)
  # test_smiles_array = np.array(test_padded_smiles)
  # train_proteins_array = np.array(train_padded_proteins)
  # test_proteins_array = np.array(test_padded_proteins)
  # train_labels_array = np.array(train_labels, dtype = "float32")
  # test_labels_array = np.array(test_labels, dtype = "float32")

  # load model
  model=load_model('medi/Model/new_DTI.h5', compile=False)

  #compile model

  optimizer = optimizers.Adam(learning_rate = 0.001)
  model.compile(loss='mse', optimizer = optimizer ,metrics=['mse'])


  # Tokenize smiles
  tokenizer_smiles = Tokenizer(char_level = True)
  tokenizer_smiles.fit_on_texts(train_smiles)

  # Tokenize proteins
  tokenizer_proteins = Tokenizer(char_level = True)
  tokenizer_proteins.fit_on_texts(train_proteins)

  def predict_pKd(drug, protein):
    # if type(drug) != list:
    #   # drug = [drug]
    drugs = drug["smiles"].values.tolist()
    # if type(protein) != list:
    #   # protein = [protein]
    proteins = protein["proteins"].values.tolist()

    print(drugs)
    drug_sequence = []
    drug_padded = []
    for d in drugs:
      drug_sequence.append(tokenizer_smiles.texts_to_sequences([d]))
      drug_padded.append(pad_sequences(tokenizer_smiles.texts_to_sequences([d]), truncating= "post", padding = "post", maxlen=85))
    # print(drug_sequence)
    # print(drug_padded)
    protein_sequence = []
    protein_padded = []
    for p in proteins:
      protein_sequence.append(tokenizer_proteins.texts_to_sequences([p]))
      protein_padded.append(pad_sequences(tokenizer_proteins.texts_to_sequences([p]), truncating = "post",padding = "post", maxlen =1200 ))
    # print(protein_sequence)
    # print(protein_padded)
    prediction = []
    for i in range(len(drug_padded)):
      pred = model.predict([tf.expand_dims(drug_padded[i], axis = -1), tf.expand_dims(protein_padded[i], axis = -1)])
      prediction.append(pred)
    # print(type(prediction))
    return(prediction)

  pred = predict_pKd(drug,protein)

  return pred